# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena

[The original paper](https://arxiv.org/pdf/2306.05685.pdf)

Evaluating LLMs, currently, is hard. Obviously, the usual criteria for choosing an evaluation metric apply - it depends firmly on its application to the task at hand. But unlike other more traditional Machine Learning use cases, we're now dealing with evaluating text data that can't really be given a hard and fast numerical translation.

When training Chat GPT, the developers decided to evaluate prompts using something called "Reinforcement Learning with Human Feedback" (RLHF). To give it a quick explanation, human experts were asked to compare two prompts and say which was better. This then gives us a "reward function" for us to encourage the model to answer prompts more like the answers that human experts said were better.

When you're not at the bleeding edge of making new "foundation models" as OpenAI et al. are, RLHF is going to be infeasible, especially with higher-ups who don't want to hire subject matter experts for what would effectively be an experiment.

## LLM-as-a-Judge

Enter "LLM-as-a-Judge". The basic idea is to pretty much ask another LLM, such as GPT4, to do the human's job in RLHF for you. This explanation might make "LLM-as-a-Judge" sound either too good to be true or a pauper's version of RLHF. It's very much the latter, as the evaluations of the paper basically show.

## How was this evaluated?

The writers of the paper made _MT-bench_, where 10 questions per category were written for eight separate categories. The categories range from maths to roleplay to general knowledge.

Additionally, Chatbot Arena gets people to compare the answers of two anonymous models. Hence this is a good place to see if LLM judgement of answers agrees with human answers.

## How did LLM-as-a-Judge compare to human judgment?

It has around 80-85% performance _at its maximum_, however this comes with two huge caveats:

* This is with GPT-4, one of the best Language models on the market at the moment however not everyone will have access to it as a judge in production due to licensing or company choice
* 85% just isn't enough to consider using this for a production-ready application.

## What issues have been recognised?

* Position Bias: physical position of the two prompts to compare actually seems to end up mattering to the "LLM-as-a-Judge"
* Verbosity Bias: the LLM Judge seems to favour longer answers
* Self-enhancement bias: LLM Judges favour answers generated by themselves
* Limited capability in grading maths and reasoning questions

You can address these by swapping positions, making the judge a few-shot judge, adding chain-of-though and reference-guided judges and fine tuning the judge model.

**NB:** This only covers an LLM comparing two prompts, something worth considering is the best way to get an LLM to score another LLM and therefore behave as a sort of numerical judge.

## Questions

* Could be compared to how GANs fight each other?